{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG: LLamaIndex Basic Tutorial\n",
    "\n",
    "#### Llama index란\n",
    "* LlamaIndex는 대규모 자연어 모델(LLM)과 함께 작동하는 데이터 프레임워크\n",
    "\n",
    "* 다양한 응용 프로그램, 예를 들어 질문 응답 시스템, 대화형 챗봇 또는 RAG를 위해 LLM과 함께 사용할 수 있습니다. \n",
    "\n",
    "* 검색 조작 증강(RAG) 메커니즘을 통해 LLM의 능력을 강화하고 사용자의 자연어 질문에 대한 응답을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LlamaIndex is a data framework that works with large natural language models (LLMs)\n",
    "\n",
    "* It can be used with LLM for a variety of applications, such as question answering systems, interactive chatbots, or RAGs.\n",
    "\n",
    "* We enhance LLM's capabilities through search manipulation augmentation (RAG) mechanisms and generate responses to users' natural language questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LlamaIndex vs Langchain\n",
    "\n",
    "| 특징              | LlamaIndex                                             | Langchain                                                                        |\n",
    "|------------------|-------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| 주요 목적         | 검색 및 검색 작업                                      | 대규모 언어 모델(LLMs)을 활용한 응용 프로그램 구축                                 |\n",
    "| 강점              | 빠르고, 효율적이며, 정확하고, 대규모 데이터 세트에 이상적이며, 간단한 인터페이스 제공 | 유연하며, 다재다능하고, 맞춤 설정이 가능하며, 다양한 LLM을 지원하고, 고급 컨텍스트 인식 기능 제공 |\n",
    "| 약점              | 검색 및 검색 작업에 한정되며, 맞춤 설정에서의 유연성이 떨어짐                   | 학습 곡선이 가파르고, 초보자에게 복잡하며, 복잡한 응용 프로그램에 대해 자원 집약적일 수 있음          |\n",
    "| 사용 사례         | 문서 검색, 코드 생성, 고객 서비스 챗봇, 콘텐츠 필터링                           | 챗봇, 가상 비서, 지식 기반, 개인화된 학습 플랫폼, 창의적 글쓰기 도구                          |\n",
    "| 사용의 용이성     | 보통                                                    | 쉬움                                                                              |\n",
    "| 문서화            | 좋음                                                    | 광범위                                                                            |\n",
    "| 비용              | 무료                                                    | 무료                                                                              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature          | LlamaIndex                                               | Langchain                                                                          |\n",
    "|------------------|----------------------------------------------------------|------------------------------------------------------------------------------------|\n",
    "| Primary Purpose  | Search and retrieval tasks                               | Building applications powered by large language models (LLMs)                      |\n",
    "| Strengths        | Fast, efficient, accurate, ideal for large data sets, simple interface | Flexible, versatile, customizable, supports diverse LLMs, advanced context-awareness |\n",
    "| Weaknesses       | Limited to search and retrieval tasks, less flexibility in customization | Steeper learning curve, complex for beginners, resource-intensive for complex applications |\n",
    "| Use Cases        | Document search, code generation, customer service chatbots, content filtering | Chatbots, virtual assistants, knowledge bases, personalized learning platforms, creative writing tools |\n",
    "| Ease of Use      | Moderate                                                 | Easy                                                                               |\n",
    "| Documentation    | Good                                                     | Extensive                                                                          |\n",
    "| Cost             | Free                                                     | Free                                                                               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex is optimized for indexing and retrieval, making it ideal for applications that demand high efficiency in these areas.   \n",
    "It is a go-to choice for applications that require efficient search and retrieval. \n",
    "\n",
    "On the other hand, Langchain is a comprehensive framework that offers a broader range of functionalities compared to LlamaIndex, which is more focused and streamlined.   \n",
    "Langchain is more flexible and customizable, allowing users to customize the application according to their needs.  \n",
    "It is particularly favored by those seeking a robust and versatile environment for their AI-driven projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAG 기본 시나리오](https://miro.medium.com/v2/resize:fit:1400/1*tAGA8bIvsul5hNyUXyib7w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Load Document](#Load)\n",
    "\n",
    "2. [Load VectorStore](#paragraph1)\n",
    "\n",
    "3. [Basic Retriever](#paragraph2)\n",
    "\n",
    "4. [Query Engine](#paragraph3)\n",
    "\n",
    "5. [Custom Prompt](#paragraph4)\n",
    "\n",
    "6. [Query LLM](#paragraph5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Document <a name=\"Load\"></a>\n",
    "\n",
    "아래 예제에서는 PDF 문서파일을 통해 지식기반 검색을 수행합니다.\n",
    "\n",
    "* 먼저 `SimpleDirectoryReader` 를 통해 문서를 로드합니다.\n",
    "\n",
    "* `SimpleDirectoryReader`는 로컬 파일의 데이터를 LlamaIndex로 로드하는 가장 간단한 방법\n",
    "\n",
    "* 기본적으로 `SimpleDirectoryReader` 찾은 모든 파일을 읽으려고 시도하여 모두 텍스트로 처리\n",
    "    - .csv - 쉼표로 구분된 값\n",
    "\n",
    "    - .docx - 마이크로소프트 워드\n",
    "    - .epub - EPUB 전자책 형식\n",
    "    - .hwp - 한글 워드 프로세서\n",
    "    - .ipynb - 주피터 노트북\n",
    "    - .jpeg, .jpg - JPEG 이미지\n",
    "    - .mbox - MBOX 이메일 아카이브\n",
    "    - .md - 마크다운\n",
    "    - .mp3, .mp4 - 오디오 및 비디오\n",
    "    - .pdf - 휴대용 문서 형식\n",
    "    - .png - 휴대용 네트워크 그래픽\n",
    "    - .ppt, .pptm, .pptx - Microsoft PowerPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "```python\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "SimpleDirectoryReader(\n",
    "    input_files=[\"path/to/file1\", \"path/to/file2\"],\n",
    "    exclude=[\"path/to/file1\", \"path/to/file2\"],\n",
    "    required_exts=[\".pdf\", \".docx\"],\n",
    "    num_files_limit=100, # 로드할 최대 파일 수를 설정할\n",
    "    encoding=\"latin-1\",\n",
    ")\n",
    "\n",
    "\"\"\"메타데이터 추출\"\"\"\n",
    "def get_meta(file_path):\n",
    "    return {\"foo\": \"bar\", \"file_path\": file_path}\n",
    "\n",
    "SimpleDirectoryReader(input_dir=\"path/to/directory\", file_metadata=get_meta)\n",
    "\n",
    "\n",
    "\"\"\"외부 파일 시스템 지원\"\"\"\n",
    "from s3fs import S3FileSystem\n",
    "\n",
    "s3_fs = S3FileSystem(key=\"...\", secret=\"...\")\n",
    "bucket_name = \"my-document-bucket\"\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=bucket_name,\n",
    "    fs=s3_fs,\n",
    "    recursive=True,  # recursively searches all subdirectories\n",
    ")\n",
    "\n",
    "documents = reader.load_data()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download articles\n",
    "> TechCrunch Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/kairess/toy-datasets/raw/master/techcrunch-articles.zip\n",
    "!unzip -q techcrunch-articles.zip -d articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"articles\")\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Techcrunch articles: 21\n",
      "Doc ID: 48151188-fb25-49c3-8ced-338e75175f9d\n",
      "Text: Signaling that investments in the supply chain sector remain\n",
      "robust, Pando, a startup developing fulfillment management\n",
      "technologies, today announced that it raised $30 million in a Series B\n",
      "round, bringing its total raised to $45 million.  Iron Pillar and\n",
      "Uncorrelated Ventures led the round, with participation from existing\n",
      "investors Nexus Vent...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Count of Techcrunch articles: {len(docs)}\")\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load한 Document를 VectorStore에 로드(Text to Vector) <a name=\"paragraph1\"></a>\n",
    "\n",
    "* `VectorStoreIndex`는 LlamaIndex에서 사용하는 데이터베이스 형식으로, 문서를 벡터로 변환합니다.\n",
    "\n",
    "* `VectorStoreIndex`는 문서가 벡터 인덱싱 되어진 스토어 입니다.\n",
    "* `VectorStoreIndex`를 통해 문서를 벡터로 변환할 때는 `Embed Model`를 통해 문서를 벡터로 변환합니다.\n",
    "*  기본적인 `Embed Model`은 OpenAI의 `text-embedding-ada-002` 입니다. (Dimesion: 1536)\n",
    "*  `VectorStoreIndex`로 document의 text가 임베딩될때 기본 청크 크기는 1024, 기본 청크 중첩은 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loaded Document to VectorStore (Text to Vector) \n",
    "\n",
    "* 'VectorStoreIndex' is a database format used by LlamaIndex that converts documents into vectors.\n",
    "\n",
    "* 'VectorStoreIndex' is the store where the document is vector indexed.\n",
    "* When converting a document into a vector through 'VectorStoreIndex', the document is converted into a vector through 'Embed Model'.\n",
    "*  The basic 'Embed Model' is OpenAI's 'text-embedding-ada-002'. (Dimesion: 1536)\n",
    "*  When the text of the document is embedded with 'VectorStoreIndex', the default chunk size is 1024, and the default chunk overlap is 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 방법1: Documents에서 VectorStoreIndex를 직접로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b03f116e2a840fbaa67c9f291430063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db75276e1a0241a78ada254c38959885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# 1. Documents에서 VectorStoreIndex를 직접로드\n",
    "# 1. Load VectorStoreIndex directly from Documents\n",
    "index = VectorStoreIndex.from_documents(docs, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 방법2: `Splitter`(chunk_size, chunk_overlap)와 embed model을 직접선택해서 `VectorStoreIndex`를 로드\n",
    "\n",
    "`방법2`는 결국 `방법1`과 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a7cfde3b48402da5473fb324638b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 2. Splitter(chunk_size, chunk_overlap)와 embed model을 직접선택해서 VectorStoreIndex를 로드\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=\"jhgan/ko-sbert-nli\",\n",
    "#     normalize=True,\n",
    "# )\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=256, chunk_overlap=20)\n",
    "nodes = node_parser.get_nodes_from_documents(docs)\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 기본 검색기 <a name=\"paragraph2\"></a>\n",
    "\n",
    "`similarity_top_k`는 유사도가 가장 높은 상위 k개의 chunk를 가져오는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "base_retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "source_nodes = base_retriever.retrieve(\"What is the CMA generative ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Score: 0.865\n",
      "The expectation is that the CMA’s Digital Markets Unit, up and running since 2021 in shadow form, will (finally) gain legislative powers in the coming years to apply pro-active “pro-competition” rules which are tailored to platforms that are deemed to have “strategic market status” (SMS). So we can speculate that providers of powerful foundational AI models may, down the line, be judged to have SMS — meaning they could expect to face bespoke rules on how they must operate vis-a-vis rivals and consumers in the U.K. market.\n",
      "\n",
      "The U.K.’s data protection watchdog, the ICO, also has its eye on generative AI. It’s another existing oversight body which the government has tasked with paying special mind to AI under its plan for context-specific guidance to steer development of the tech through the application of existing laws.\n",
      "\n",
      "In a blog post last month, Stephen Almond, the ICO’s executive director of regulatory risk, offered some tips and a little warning for developers of generative AI when it comes to compliance with U.K.\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Score: 0.862\n",
      "Well that was fast. The U.K.’s competition watchdog has announced an initial review of “AI foundational models”, such as the large language models (LLMs) which underpin OpenAI’s ChatGPT and Microsoft’s New Bing. Generative AI models which power AI art platforms such as OpenAI’s DALL-E or Midjourney will also likely fall in scope.\n",
      "\n",
      "The Competition and Markets Authority (CMA) said its review will look at competition and consumer protection considerations in the development and use of AI foundational models — with the aim of understanding “how foundation models are developing and producing an assessment of the conditions and principles that will best guide the development of foundation models and their use in the future”.\n",
      "\n",
      "It’s proposing to publish the review in “early September”, with a deadline of June 2 for interested stakeholders to submit responses to inform its work.\n",
      "\n",
      "“Foundation models, which include large language models and generative artificial intelligence (AI), that have emerged over the past five years, have the potential to transform much of what people and businesses do.\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Score: 0.854\n",
      "The CMA says its initial review of foundational AI models is in line with instructions in the white paper, where the government talked about existing regulators conducting “detailed risk analysis” in order to be in a position to carry out potential enforcements, i.e. on dangerous, unfair and unaccountable applications of AI, using their existing powers.\n",
      "\n",
      "The regulator also points to its core mission — to support open, competitive markets — as another reason for taking a look at generative AI now.\n",
      "\n",
      "Notably, the competition watchdog is set to get additional powers to regulate Big Tech in the coming years, under plans taken off the back-burner by prime minister Rishi Sunak’s government last month, when ministers said it would move forward with a long-trailed (but much delayed) ex ante reform aimed at digital giants’ market power.\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Score: 0.849\n",
      "To ensure that innovation in AI continues in a way that benefits consumers, businesses and the UK economy, the government has asked regulators, including the [CMA], to think about how the innovative development and deployment of AI can be supported against five overarching principles: safety, security and robustness; appropriate transparency and explainability; fairness; accountability and governance; and contestability and redress,” the CMA wrote in a press release.”\n",
      "\n",
      "Stanford University’s Human-Centered Artificial Intelligence Center’s Center for Research on Foundation Models is credited with coining the term “foundational models”, back in 2021, to refer to AI systems that focus on training one model on a huge amount of data and adapting it to many applications.\n",
      "\n",
      "“The development of AI touches upon a number of important issues, including safety, security, copyright, privacy, and human rights, as well as the ways markets work.\n",
      "---------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "Score: 0.848\n",
      "Many of these issues are being considered by government or other regulators, so this initial review will focus on the questions the CMA is best placed to address — what are the likely implications of the development of AI foundation models for competition and consumer protection?” the CMA added.\n",
      "\n",
      "In a statement, its CEO, Sarah Cardell, also said:\n",
      "\n",
      "AI has burst into the public consciousness over the past few months but has been on our radar for some time. It’s a technology developing at speed and has the potential to transform the way businesses compete as well as drive substantial economic growth. It’s crucial that the potential benefits of this transformative technology are readily accessible to UK businesses and consumers while people remain protected from issues like false or misleading information. Our goal is to help this new, rapidly scaling technology develop in ways that ensure open, competitive markets and effective consumer protection.\n",
      "\n",
      "Specifically, the U.K.\n",
      "---------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in source_nodes:\n",
    "    # print(node.metadata)\n",
    "    print(f\"---------------------------------------------\")\n",
    "    print(f\"Score: {node.score:.3f}\")\n",
    "    print(node.get_content())\n",
    "    print(f\"---------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 쿼리엔진 <a name=\"paragraph3\"></a>\n",
    "\n",
    "쿼리엔진에서 사용하는 기본 LLM model은 OpenAI `gpt-3.5-turbo`, `temperature` 는 0.1 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "# llm = OpenAI(model=\"gpt-4\",temperature=0)\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM 커스텀 프롬프트 세팅 <a name=\"paragraph4\"></a>\n",
    "\n",
    "LLM 커스텀 프롬프트 세팅은 `LlamaIndex`에서 제공하는 `PromptTemplate`을 통해 프롬프트를 세팅합니다.\n",
    "\n",
    "`PromptTemplate`을 작성할때 `context_str`과 `query_str`은 외부에서 주입해주는 변수입니다.\n",
    "- context_str: 청크된 문서의 내용\n",
    "- query_str: 사용자의 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "prompt_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query.\\n\"\n",
    "    \"You MUST answer in Korean.\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "prompt_tmpl = PromptTemplate(prompt_tmpl_str)\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))\n",
    "\n",
    "\n",
    "dict = query_engine.get_prompts()\n",
    "display_prompt_dict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 쿼리실행 <a name=\"paragraph5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "The CMA generative AI refers to generative artificial intelligence models that are being reviewed by the Competition and Markets Authority (CMA) in the UK. These models include large language models and generative AI technologies like those powering AI art platforms such as OpenAI’s DALL-E or Midjourney."
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the CMA generative ai?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Text:\t The expectation is that the CMA’s Digital Markets Unit, up and running since 2021 in shadow form, will (finally) gain legislative powers in the coming years to apply pro-active “pro-competition” rules which are tailored to platforms that are deemed to have “strategic market status” (SMS). So we can speculate that providers of powerful foundational AI models may, down the line, be judged to have SMS — meaning they could expect to face bespoke rules on how they must operate vis-a-vis rivals and consumers in the U.K. market.  The U.K.’s data protection watchdog, the ICO, also has its eye on generative AI. It’s another existing oversight body which the government has tasked with paying special mind to AI under its plan for context-specific guidance to steer development of the tech through the application of existing laws.  In a blog post last month, Stephen Almond, the ICO’s executive director of regulatory risk, offered some tips and a little warning for developers of generative AI when i ...\n",
      "Metadata:\t {'file_path': '/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Basic Tutorial/articles/05-04-cma-generative-ai-review.txt', 'file_name': '05-04-cma-generative-ai-review.txt', 'file_type': 'text/plain', 'file_size': 7607, 'creation_date': '2024-04-08', 'last_modified_date': '2023-05-08'}\n",
      "Score:\t 0.865\n",
      "-----\n",
      "Text:\t Well that was fast. The U.K.’s competition watchdog has announced an initial review of “AI foundational models”, such as the large language models (LLMs) which underpin OpenAI’s ChatGPT and Microsoft’s New Bing. Generative AI models which power AI art platforms such as OpenAI’s DALL-E or Midjourney will also likely fall in scope.  The Competition and Markets Authority (CMA) said its review will look at competition and consumer protection considerations in the development and use of AI foundational models — with the aim of understanding “how foundation models are developing and producing an assessment of the conditions and principles that will best guide the development of foundation models and their use in the future”.  It’s proposing to publish the review in “early September”, with a deadline of June 2 for interested stakeholders to submit responses to inform its work.  “Foundation models, which include large language models and generative artificial intelligence (AI), that have emerg ...\n",
      "Metadata:\t {'file_path': '/Users/heewungsong/Experiment/Visa_Rag/study/llama-index/Basic Tutorial/articles/05-04-cma-generative-ai-review.txt', 'file_name': '05-04-cma-generative-ai-review.txt', 'file_type': 'text/plain', 'file_size': 7607, 'creation_date': '2024-04-08', 'last_modified_date': '2023-05-08'}\n",
      "Score:\t 0.862\n"
     ]
    }
   ],
   "source": [
    "for node in response.source_nodes:\n",
    "    print(\"-----\")\n",
    "    text_fmt = node.node.get_content().strip().replace(\"\\n\", \" \")[:1000]\n",
    "    print(f\"Text:\\t {text_fmt} ...\")\n",
    "    print(f\"Metadata:\\t {node.node.metadata}\")\n",
    "    print(f\"Score:\\t {node.score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyautogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
